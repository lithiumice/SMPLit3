# -*- coding: utf-8 -*-
#
# Copyright (C) 2021 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# AMASS: Archive of Motion Capture as Surface Shapes <https://arxiv.org/abs/1904.03278>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2019.08.09
import os
import sys

file_path = os.path.dirname(os.path.realpath(__file__))
sys.path.append(os.path.join(file_path, ".."))

import glob
from datetime import datetime

import holden.BVH as BVH
import numpy as np
import torch
from arguments import Arguments
from holden.Animation import Animation
from holden.Quaternions import Quaternions
from human_body_prior.tools.omni_tools import copy2cpu as c2c
from human_body_prior.tools.omni_tools import log2file, makepath
from nemf.fk import ForwardKinematicsLayer
from nemf_rotations import (
    axis_angle_to_matrix,
    axis_angle_to_quaternion,
    matrix_to_rotation_6d,
    rotation_6d_to_matrix,
)
from torch.utils.data import Dataset
from tqdm import tqdm
from nemf_utils import (
    CONTACTS_IDX,
    align_joints,
    build_canonical_frame,
    estimate_angular_velocity,
    estimate_linear_velocity,
    normalize,
)


def dump_amass2pytroch(datasets, amass_dir, out_posepath, logger=None):
    makepath(out_posepath, isfile=True)

    if logger is None:
        starttime = datetime.now().replace(microsecond=0)
        log_name = datetime.strftime(starttime, "%Y%m%d_%H%M")
        logger = log2file(
            out_posepath.replace(
                f"pose-{args.data.gender}-{args.data.clip_length}-{args.data.fps}fps.pt",
                "%s.log" % (log_name),
            )
        )
        logger("Creating pytorch dataset at %s" % out_posepath)

    data_poses = []
    data_trans = []
    data_contacts = []

    clip_frames = args.data.clip_length
    for ds_name in datasets:
        npz_fnames = glob.glob(os.path.join(amass_dir, ds_name, "*/*_poses_*.npz"))
        logger("processing data points from %s." % (ds_name))
        for npz_fname in tqdm(npz_fnames):
            try:
                cdata = np.load(npz_fname)
            except:
                logger("Could not read %s! skipping.." % npz_fname)
                continue
            N = len(cdata["pose_body"])
            gender = str(cdata["gender"])
            if gender.startswith("b"):
                gender = str(cdata["gender"], encoding="utf-8")

            # Only process data with the specific gender.
            if gender != args.data.gender:
                continue

            root_orient = cdata["root_orient"]
            pose_body = cdata["pose_body"]
            poses = np.concatenate((root_orient, pose_body, np.zeros((N, 6))), axis=1)
            # Chop the data into evenly splitted sequence clips.
            nclips = [np.arange(i, i + clip_frames) for i in range(0, N, clip_frames)]
            if N % clip_frames != 0:
                nclips.pop()
            for clip in nclips:
                data_poses.append(poses[clip])
                data_trans.append(cdata["trans"][clip])
                data_contacts.append(cdata["contacts"][clip])
                # if ds_name in ['MPI_HDM05', 'SFU', 'MPI_mosh']:
                #     print(npz_fname)

    assert len(data_poses) != 0

    # Choose the device to run the body model on.
    # device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    device = torch.device("cpu")  # when GPU memory is limited
    pose = torch.from_numpy(np.asarray(data_poses, np.float32)).to(device)
    pose = pose.view(-1, clip_frames, 24, 3)  # axis-angle (N, T, J, 3)
    trans = torch.from_numpy(np.asarray(data_trans, np.float32)).to(
        device
    )  # global translation (N, T, 3)

    # Compute necessary data for model training.
    rotmat = axis_angle_to_matrix(pose)  # rotation matrix (N, T, J, 3, 3)
    root_orient = rotmat[:, :, 0].clone()
    root_orient = matrix_to_rotation_6d(root_orient)  # root orientation (N, T, 6)
    if args.unified_orientation:
        identity = torch.zeros_like(rotmat[:, :, 0])  # (N, T, 3, 3)
        identity[:, :, 0, 0] = 1
        identity[:, :, 1, 1] = 1
        identity[:, :, 2, 2] = 1
        rotmat[:, :, 0] = identity

    rot6d = matrix_to_rotation_6d(rotmat)  # 6D rotation representation (N, T, J, 6)
    rot_seq = rotmat.clone()
    angular = estimate_angular_velocity(
        rot_seq, dt=1.0 / args.data.fps
    )  # angular velocity of all the joints (N, T, J, 3)

    fk = ForwardKinematicsLayer(args, device=device)
    pos, global_xform = fk(rot6d.view(-1, 24, 6))
    pos = pos.contiguous().view(
        -1, clip_frames, 24, 3
    )  # local joint positions (N, T, J, 3)
    global_xform = global_xform.view(
        -1, clip_frames, 24, 4, 4
    )  # global transformation matrix for each joint (N, T, J, 4, 4)
    velocity = estimate_linear_velocity(
        pos, dt=1.0 / args.data.fps
    )  # linear velocity of all the joints (N, T, J, 3)

    contacts = torch.from_numpy(np.asarray(data_contacts, np.float32)).to(device)
    contacts = contacts[:, :, CONTACTS_IDX]  # contacts information (N, T, 8)

    if args.unified_orientation:
        root_rotation = rotation_6d_to_matrix(root_orient)  # (N, T, 3, 3)
        root_rotation = root_rotation.unsqueeze(2).repeat(
            1, 1, args.smpl.joint_num, 1, 1
        )  # (N, T, J, 3, 3)
        global_pos = torch.matmul(root_rotation, pos.unsqueeze(-1)).squeeze(-1)
        height = global_pos + trans.unsqueeze(2)
    else:
        height = pos + trans.unsqueeze(2)
    height = height[:, :, :, "xyz".index(args.data.up)]  # (N, T, J)
    root_vel = estimate_linear_velocity(
        trans, dt=1.0 / args.data.fps
    )  # linear velocity of the root joint (N, T, 3)

    _, rotmat_mean, rotmat_std = normalize(rotmat)
    _, pos_mean, pos_std = normalize(pos)
    _, trans_mean, trans_std = normalize(trans)
    _, root_vel_mean, root_vel_std = normalize(root_vel)
    _, height_mean, height_std = normalize(height)
    _, contacts_mean, contacts_std = normalize(contacts)

    mean = {
        "rotmat": rotmat_mean.detach().cpu(),
        "pos": pos_mean.detach().cpu(),
        "trans": trans_mean.detach().cpu(),
        "root_vel": root_vel_mean.detach().cpu(),
        "height": height_mean.detach().cpu(),
        "contacts": contacts_mean.detach().cpu(),
    }
    std = {
        "rotmat": rotmat_std.detach().cpu(),
        "pos": pos_std.detach().cpu(),
        "trans": trans_std.detach().cpu(),
        "root_vel": root_vel_std.detach().cpu(),
        "height": height_std.detach().cpu(),
        "contacts": contacts_std.detach().cpu(),
    }

    torch.save(
        rotmat.detach().cpu(), out_posepath.replace("pose", "rotmat")
    )  # (N, T, J, 3, 3)
    torch.save(pos.detach().cpu(), out_posepath.replace("pose", "pos"))  # (N, T, J, 3)
    torch.save(trans.detach().cpu(), out_posepath.replace("pose", "trans"))  # (N, T, 3)
    torch.save(
        root_vel.detach().cpu(), out_posepath.replace("pose", "root_vel")
    )  # (N, T, 3)
    torch.save(
        height.detach().cpu(), out_posepath.replace("pose", "height")
    )  # (N, T, J)
    torch.save(
        contacts.detach().cpu(), out_posepath.replace("pose", "contacts")
    )  # (N, T, J)

    if args.canonical:
        forward = rotmat[:, :, 0, :, 2].clone()
        canonical_frame = build_canonical_frame(forward, up_axis=args.data.up)
        root_rotation = canonical_frame.transpose(-2, -1)  # (N, T, 3, 3)
        root_rotation = root_rotation.unsqueeze(2).repeat(
            1, 1, args.smpl.joint_num, 1, 1
        )  # (N, T, J, 3, 3)

        theta = torch.atan2(forward[..., 1], forward[..., 0])
        dt = 1.0 / args.data.fps
        forward_ang = (theta[:, 1:] - theta[:, :-1]) / dt
        forward_ang = torch.cat((forward_ang, forward_ang[..., -1:]), dim=-1)  # (N, T)

        local_pos = torch.matmul(root_rotation, pos.unsqueeze(-1)).squeeze(-1)
        local_vel = torch.matmul(root_rotation, velocity.unsqueeze(-1)).squeeze(-1)
        local_rot = torch.matmul(root_rotation, global_xform[:, :, :, :3, :3])
        local_rot = matrix_to_rotation_6d(local_rot)
        local_ang = torch.matmul(root_rotation, angular.unsqueeze(-1)).squeeze(-1)

        _, forward_mean, forward_std = normalize(forward)
        _, forward_ang_mean, forward_ang_std = normalize(forward_ang)
        _, local_pos_mean, local_pos_std = normalize(local_pos)
        _, local_vel_mean, local_vel_std = normalize(local_vel)
        _, local_rot_mean, local_rot_std = normalize(local_rot)
        _, local_ang_mean, local_ang_std = normalize(local_ang)

        mean["forward"] = forward_mean.detach().cpu()
        mean["forward_ang"] = forward_ang_mean.detach().cpu()
        mean["local_pos"] = local_pos_mean.detach().cpu()
        mean["local_vel"] = local_vel_mean.detach().cpu()
        mean["local_rot"] = local_rot_mean.detach().cpu()
        mean["local_ang"] = local_ang_mean.detach().cpu()

        std["forward"] = forward_std.detach().cpu()
        std["forward_ang"] = forward_ang_std.detach().cpu()
        std["local_pos"] = local_pos_std.detach().cpu()
        std["local_vel"] = local_vel_std.detach().cpu()
        std["local_rot"] = local_rot_std.detach().cpu()
        std["local_ang"] = local_ang_std.detach().cpu()

        torch.save(
            forward.detach().cpu(), out_posepath.replace("pose", "forward")
        )  # (N, T, 3)
        torch.save(
            local_pos.detach().cpu(), out_posepath.replace("pose", "local_pos")
        )  # (N, T, J, 3)
        torch.save(
            local_vel.detach().cpu(), out_posepath.replace("pose", "local_vel")
        )  # (N, T, J, 3)
        torch.save(
            local_rot.detach().cpu(), out_posepath.replace("pose", "local_rot")
        )  # (N, T, J, 6)
        torch.save(
            local_ang.detach().cpu(), out_posepath.replace("pose", "local_ang")
        )  # (N, T, J, 3)
    else:
        global_xform = global_xform[:, :, :, :3, :3]  # (N, T, J, 3, 3)
        global_xform = matrix_to_rotation_6d(global_xform)  # (N, T, J, 6)

        _, rot6d_mean, rot6d_std = normalize(rot6d)
        _, angular_mean, angular_std = normalize(angular)
        _, global_xform_mean, global_xform_std = normalize(global_xform)
        _, velocity_mean, velocity_std = normalize(velocity)
        _, orientation_mean, orientation_std = normalize(root_orient)

        mean["rot6d"] = rot6d_mean.detach().cpu()
        mean["angular"] = angular_mean.detach().cpu()
        mean["global_xform"] = global_xform_mean.detach().cpu()
        mean["velocity"] = velocity_mean.detach().cpu()
        mean["root_orient"] = orientation_mean.detach().cpu()

        std["rot6d"] = rot6d_std.detach().cpu()
        std["angular"] = angular_std.detach().cpu()
        std["global_xform"] = global_xform_std.detach().cpu()
        std["velocity"] = velocity_std.detach().cpu()
        std["root_orient"] = orientation_std.detach().cpu()

        torch.save(
            rot6d.detach().cpu(), out_posepath.replace("pose", "rot6d")
        )  # (N, T, J, 6)
        torch.save(
            angular.detach().cpu(), out_posepath.replace("pose", "angular")
        )  # (N, T, J, 3)
        torch.save(
            global_xform.detach().cpu(), out_posepath.replace("pose", "global_xform")
        )  # (N, T, J, 6)
        torch.save(
            velocity.detach().cpu(), out_posepath.replace("pose", "velocity")
        )  # (N, T, J, 3)
        torch.save(
            root_orient.detach().cpu(), out_posepath.replace("pose", "root_orient")
        )  # (N, T, 3, 3)

    if args.normalize:
        torch.save(mean, out_posepath.replace("pose", "mean"))
        torch.save(std, out_posepath.replace("pose", "std"))

    return len(data_poses)


def dump_amass2single(datasets, amass_dir, split_name, logger=None, bvh_viz=False):
    # Choose the device to run the body model on.
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    fk = ForwardKinematicsLayer(args)

    index = 0
    for ds_name in datasets:
        npz_fnames = sorted(
            glob.glob(os.path.join(amass_dir, ds_name, "*/*_poses_*.npz"))
        )
        logger("processing data points from %s." % (ds_name))
        for npz_fname in tqdm(npz_fnames):
            try:
                cdata = np.load(npz_fname)
            except:
                logger("Could not read %s! skipping.." % npz_fname)
                continue

            gender = str(cdata["gender"])
            if gender.startswith("b"):
                gender = str(cdata["gender"], encoding="utf-8")

            # Only process data with the specific gender.
            if gender != args.data.gender:
                continue

            N = len(cdata["pose_body"])
            logger(f"Sequence {index} has {N} frames")

            root_orient = cdata["root_orient"]
            pose_body = cdata["pose_body"]
            data_poses = np.concatenate(
                (root_orient, pose_body, np.zeros((N, 6))), axis=1
            )
            pose = torch.from_numpy(np.asarray(data_poses, np.float32)).to(device)
            pose = pose.view(-1, 24, 3)  # axis-angle (T, J, 3)
            trans = torch.from_numpy(np.asarray(cdata["trans"], np.float32)).to(
                device
            )  # global translation (T, 3)

            # Compute necessary data for model training.
            rotmat = axis_angle_to_matrix(pose)  # rotation matrix (T, J, 3, 3)
            root_orient = rotmat[:, 0].clone()
            root_orient = matrix_to_rotation_6d(root_orient)  # root orientation (T, 6)
            if args.unified_orientation:
                identity = torch.eye(3).cuda()
                identity = identity.view(1, 3, 3).repeat(rotmat.shape[0], 1, 1)
                rotmat[:, 0] = identity
            rot6d = matrix_to_rotation_6d(
                rotmat
            )  # 6D rotation representation (T, J, 6)

            rot_seq = rotmat.clone()
            angular = estimate_angular_velocity(
                rot_seq.unsqueeze(0), dt=1.0 / args.data.fps
            ).squeeze(
                0
            )  # angular velocity of all the joints (T, J, 3)

            pos, global_xform = fk(
                rot6d
            )  # local joint positions (T, J, 3), global transformation matrix for each joint (T, J, 4, 4)
            pos = pos.contiguous()
            global_xform = global_xform.contiguous()
            velocity = estimate_linear_velocity(
                pos.unsqueeze(0), dt=1.0 / args.data.fps
            ).squeeze(
                0
            )  # linear velocity of all the joints (T, J, 3)

            contacts = torch.from_numpy(np.asarray(cdata["contacts"], np.float32)).to(
                device
            )
            contacts = contacts[:, CONTACTS_IDX]  # contacts information (T, 8)

            if args.unified_orientation:
                root_rotation = rotation_6d_to_matrix(root_orient)  # (T, 3, 3)
                root_rotation = root_rotation.unsqueeze(1).repeat(
                    1, args.smpl.joint_num, 1, 1
                )  # (T, J, 3, 3)
                global_pos = torch.matmul(root_rotation, pos.unsqueeze(-1)).squeeze(-1)
                height = global_pos + trans.unsqueeze(1)
            else:
                height = pos + trans.unsqueeze(1)
            height = height[..., "xyz".index(args.data.up)]  # (T, J)
            root_vel = estimate_linear_velocity(
                trans.unsqueeze(0), dt=1.0 / args.data.fps
            ).squeeze(
                0
            )  # linear velocity of the root joint (T, 3)

            out_posepath = os.path.join(work_dir, split_name, f"trans_{index}.pt")
            torch.save(trans.detach().cpu(), out_posepath)  # (T, 3)
            torch.save(
                root_vel.detach().cpu(), out_posepath.replace("trans", "root_vel")
            )  # (T, 3)
            torch.save(
                pos.detach().cpu(), out_posepath.replace("trans", "pos")
            )  # (T, J, 3)
            torch.save(
                rotmat.detach().cpu(), out_posepath.replace("trans", "rotmat")
            )  # (T, J, 3, 3)
            torch.save(
                height.detach().cpu(), out_posepath.replace("trans", "height")
            )  # (T, J)
            torch.save(
                contacts.detach().cpu(), out_posepath.replace("trans", "contacts")
            )  # (T, J)

            if args.canonical:
                forward = rotmat[:, 0, :, 2].clone()
                canonical_frame = build_canonical_frame(forward, up_axis=args.data.up)
                root_rotation = canonical_frame.transpose(-2, -1)  # (T, 3, 3)
                root_rotation = root_rotation.unsqueeze(1).repeat(
                    1, args.smpl.joint_num, 1, 1
                )  # (T, J, 3, 3)

                if args.data.up == "x":
                    theta = torch.atan2(forward[..., 2], forward[..., 1])
                elif args.data.up == "y":
                    theta = torch.atan2(forward[..., 0], forward[..., 2])
                else:
                    theta = torch.atan2(forward[..., 1], forward[..., 0])
                dt = 1.0 / args.data.fps
                forward_ang = (theta[1:] - theta[:-1]) / dt
                forward_ang = torch.cat((forward_ang, forward_ang[-1:]), dim=-1)

                local_pos = torch.matmul(root_rotation, pos.unsqueeze(-1)).squeeze(-1)
                local_vel = torch.matmul(root_rotation, velocity.unsqueeze(-1)).squeeze(
                    -1
                )
                local_rot = torch.matmul(root_rotation, global_xform[:, :, :3, :3])
                local_rot = matrix_to_rotation_6d(local_rot)
                local_ang = torch.matmul(root_rotation, angular.unsqueeze(-1)).squeeze(
                    -1
                )

                torch.save(
                    forward.detach().cpu(), out_posepath.replace("trans", "forward")
                )
                torch.save(
                    forward_ang.detach().cpu(),
                    out_posepath.replace("trans", "forward_ang"),
                )
                torch.save(
                    local_pos.detach().cpu(), out_posepath.replace("trans", "local_pos")
                )
                torch.save(
                    local_vel.detach().cpu(), out_posepath.replace("trans", "local_vel")
                )
                torch.save(
                    local_rot.detach().cpu(), out_posepath.replace("trans", "local_rot")
                )
                torch.save(
                    local_ang.detach().cpu(), out_posepath.replace("trans", "local_ang")
                )
            else:
                global_xform = global_xform[:, :, :3, :3]  # (T, J, 3, 3)
                global_xform = matrix_to_rotation_6d(global_xform)  # (T, J, 6)

                torch.save(
                    rot6d.detach().cpu(), out_posepath.replace("trans", "rot6d")
                )  # (N, T, J, 6)
                torch.save(
                    angular.detach().cpu(), out_posepath.replace("trans", "angular")
                )  # (N, T, J, 3)
                torch.save(
                    global_xform.detach().cpu(),
                    out_posepath.replace("trans", "global_xform"),
                )  # (N, T, J, 6)
                torch.save(
                    velocity.detach().cpu(), out_posepath.replace("trans", "velocity")
                )  # (N, T, J, 3)
                torch.save(
                    root_orient.detach().cpu(),
                    out_posepath.replace("trans", "root_orient"),
                )  # (N, T, 3, 3)

            if bvh_viz:
                bvh_fname = os.path.join(
                    work_dir, "bvh", split_name, f"pose_{index}.bvh"
                )
                makepath(bvh_fname, isfile=True)
                rotation = axis_angle_to_quaternion(pose)
                rotation = align_joints(rotation)
                position = trans.unsqueeze(1)
                # Export the motion data to .bvh file with SMPL skeleton.
                anim = Animation(
                    Quaternions(c2c(rotation)),
                    c2c(position),
                    None,
                    offsets=args.smpl.offsets[gender],
                    parents=args.smpl.parents,
                )
                BVH.save(
                    bvh_fname,
                    anim,
                    names=args.smpl.joint_names,
                    frametime=1 / args.data.fps,
                )

            index += 1

    return index


def prepare_amass(amass_splits, amass_dir, work_dir, logger=None):

    if logger is None:
        starttime = datetime.now().replace(microsecond=0)
        log_name = datetime.strftime(starttime, "%Y%m%d_%H%M")
        logger = log2file(os.path.join(work_dir, "%s.log" % (log_name)))
        logger("Creating pytorch dataset at %s" % work_dir)

    logger(
        "Fetch data from AMASS npz files, augment the data and dump every data field as final pytorch pt files"
    )

    for split_name, datasets in amass_splits.items():
        outpath = makepath(
            os.path.join(
                work_dir,
                split_name,
                f"pose-{args.data.gender}-{args.data.clip_length}-{args.data.fps}fps.pt",
            ),
            isfile=True,
        )
        if os.path.exists(outpath):
            continue
        if args.single_motion:
            ndatapoints = dump_amass2single(
                datasets, amass_dir, split_name, logger=logger, bvh_viz=True
            )
        else:
            ndatapoints = dump_amass2pytroch(
                datasets, amass_dir, outpath, logger=logger
            )
        logger("%s has %d data points!" % (split_name, ndatapoints))


def dump_amass2bvh(amass_splits, amass_dir, out_bvhpath, logger=None):
    from nemf_utils import export_bvh_animation

    if logger is None:
        log_name = os.path.join(out_bvhpath, "bvh_viz.log")
        if os.path.exists(log_name):
            os.remove(log_name)
        logger = log2file(log_name)
        logger("Creating bvh visualization at %s" % out_bvhpath)

    for split_name, datasets in amass_splits.items():
        outpath = makepath(os.path.join(out_bvhpath, split_name))
        if os.path.exists(outpath) and len(os.listdir(outpath)) != 0:
            continue

        for ds_name in datasets:
            npz_fnames = glob.glob(os.path.join(amass_dir, ds_name, "*/*_poses_*.npz"))
            logger("visualizing data points at %s." % (ds_name))
            for npz_fname in tqdm(npz_fnames):
                bvh_fname = os.path.split(npz_fname)[1].replace(".npz", ".bvh")
                try:
                    export_bvh_animation(
                        npz_fname, os.path.join(outpath, ds_name + "_" + bvh_fname)
                    )
                except AssertionError:
                    logger(
                        "Could not export %s because its gender is not supported! skipping.."
                        % npz_fname
                    )
                    continue


def collect_amass_stats(amass_splits, amass_dir, logger=None):
    import matplotlib.pyplot as plt

    if logger is None:
        log_name = os.path.join("./data/amass", "amass_stats.log")
        if os.path.exists(log_name):
            os.remove(log_name)
        logger = log2file(log_name)

    logger("Collecting stats for AMASS datasets:")

    gender_stats = {}
    fps_stats = {}
    durations = []
    for split_name, datasets in amass_splits.items():
        for ds_name in datasets:
            logger(f"\t{ds_name} dataset")
            npz_fnames = glob.glob(os.path.join(amass_dir, ds_name, "*/*_poses_*.npz"))
            for npz_fname in tqdm(npz_fnames):
                try:
                    cdata = np.load(npz_fname)
                except:
                    logger("Could not read %s! skipping.." % npz_fname)
                    continue

                gender = str(cdata["gender"])
                if gender.startswith("b"):
                    gender = str(cdata["gender"], encoding="utf-8")
                fps = 30
                duration = len(cdata["pose_body"]) / fps

                durations.append(duration)
                if gender in gender_stats.keys():
                    gender_stats[gender].append(duration)
                else:
                    gender_stats[gender] = [duration]
                if fps in fps_stats.keys():
                    fps_stats[fps] += 1
                else:
                    fps_stats[fps] = 1

    logger("\n")
    logger("Total motion sequences: {:,}".format(len(durations)))
    logger("\tTotal Duration: {:,.2f}s".format(sum(durations)))
    logger("\tMin Duration: {:,.2f}s".format(min(durations)))
    logger("\tMax Duration: {:,.2f}s".format(max(durations)))
    logger("\tAverage Duration: {:,.2f}s".format(sum(durations) / len(durations)))
    logger(
        "\tSequences longer than 5s: {:,} ({:.2f}%)".format(
            sum(i > 5 for i in durations),
            sum(i > 5 for i in durations) / len(durations) * 100,
        )
    )
    logger(
        "\tSequences longer than 10s: {:,} ({:.2f}%)".format(
            sum(i > 10 for i in durations),
            sum(i > 10 for i in durations) / len(durations) * 100,
        )
    )
    logger("\n")
    logger("Gender:")
    for key, value in gender_stats.items():
        logger("\t{}: {:,} (Duration: {:,.2f}s)".format(key, len(value), sum(value)))
    logger("\n")
    logger("FPS:")
    for key, value in fps_stats.items():
        logger("\t{}: {:,}".format(key, value))

    # Plot histograms for duration distributions.
    fig, axes = plt.subplots(3, sharex=True)
    fig.tight_layout(pad=3.0)
    fig.suptitle("AMASS Data Duration Distribution")
    axes[0].hist(durations, density=False, bins=100)
    axes[0].set_title("total")
    for key, value in gender_stats.items():
        if key == "female":
            f = axes[1]
        else:
            f = axes[2]
        f.hist(value, density=False, bins=100)
        f.set_title(f"{key}")
    # Add a big axis, hide frame.
    fig.add_subplot(111, frameon=False)
    # Hide tick and tick label of the big axis.
    plt.tick_params(
        labelcolor="none",
        which="both",
        top=False,
        bottom=False,
        left=False,
        right=False,
    )
    plt.xlabel("Duration (second)")
    plt.ylabel("Counts", labelpad=10)
    fig.savefig("./data/amass/duration_dist.pdf")


class AMASS(Dataset):
    """AMASS: a pytorch loader for unified human motion capture dataset. http://amass.is.tue.mpg.de/"""

    def __init__(self, dataset_dir):
        self.ds = {}
        for data_fname in glob.glob(os.path.join(dataset_dir, "*.pt")):
            k = os.path.basename(data_fname).split("-")[0]
            self.ds[k] = torch.load(data_fname)

    def __len__(self):
        return len(self.ds["trans"])

    def __getitem__(self, idx):
        data = {k: self.ds[k][idx] for k in self.ds.keys() if k not in ["mean", "std"]}

        return data


if __name__ == "__main__":
    msg = """ Using standard AMASS dataset preparation pipeline: 
    0) Donwload all npz files from https://amass.is.tue.mpg.de/ 
    1) Convert npz files to pytorch readable pt files. """

    amass_dir = "../AMASS_processed/"

    args = Arguments("./configs", filename="amass.yaml")
    work_dir = makepath(args.dataset_dir)

    log_name = os.path.join(work_dir, "amass.log")
    if os.path.exists(log_name):
        os.remove(log_name)
    logger = log2file(log_name)
    logger("AMASS Data Preparation Began.")
    logger(msg)

    if args.single_motion:
        amass_splits = {"train": ["DanceDB"]}
    else:
        amass_splits = {
            "valid": ["MPI_HDM05", "SFU", "MPI_mosh"],
            "test": ["Transitions_mocap", "HumanEva"],
            "train": [
                "CMU",
                "MPI_Limits",
                "TotalCapture",
                "Eyes_Japan_Dataset",
                "KIT",
                "BioMotionLab_NTroje",
                "BMLmovi",
                "EKUT",
                "ACCAD",
                "BMLhandball",
                "DanceDB",
                "DFaust_67",
                "SSM_synced",
            ],
        }
        # amass_splits = {
        #     'no_split': ['MPI_HDM05', 'SFU', 'MPI_mosh', 'Transitions_mocap', 'HumanEva', 'CMU', 'MPI_Limits', 'TotalCapture', 'Eyes_Japan_Dataset', 'KIT', 'BioMotionLab_NTroje', 'BMLmovi', 'EKUT', 'ACCAD', 'BMLhandball', 'DanceDB', 'DFaust_67', 'SSM_synced']
        # }

    # collect_amass_stats(amass_splits, amass_dir)
    prepare_amass(amass_splits, amass_dir, work_dir, logger=logger)
