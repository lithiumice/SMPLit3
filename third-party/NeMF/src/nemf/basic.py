import os

import holden.BVH as BVH
import numpy as np
import torch
import torch.nn as nn
from holden.Animation import Animation
from holden.Quaternions import Quaternions
from human_body_prior.tools.omni_tools import copy2cpu as c2c
from nemf_rotations import (
    matrix_to_quaternion,
    quaternion_to_axis_angle,
    rotation_6d_to_matrix,
)
from nemf_utils import align_joints, compute_trajectory

from .base_model import BaseModel
from .fk import ForwardKinematicsLayer
from .losses import GeodesicLoss
from .neural_motion import NeuralMotionField


class Architecture(BaseModel):
    def __init__(self, args, ngpu):
        super(Architecture, self).__init__(args)

        self.args = args

        if args.amass_data:
            self.fk = ForwardKinematicsLayer(args)
        else:
            self.fk = ForwardKinematicsLayer(
                parents=args.animal.parents, positions=args.animal.offsets
            )

        self.field = NeuralMotionField(args.nemf).to(self.device)
        if args.multi_gpu is True:
            self.field = nn.DataParallel(self.field, device_ids=range(ngpu))

        self.models = [self.field]

        ordermap = {
            "x": 0,
            "y": 1,
            "z": 2,
        }
        self.up_axis = ordermap[self.args.data.up]
        self.v_axis = [x for x in ordermap.values() if x != self.up_axis]

        # print(self.up_axis)
        # print(self.v_axis)

        self.input_data = dict()
        self.recon_data = dict()

        if self.is_train:
            self.optimizer = torch.optim.Adam(
                self.field.parameters(), args.learning_rate
            )
            self.optimizers = [self.optimizer]
            self.criterion_rec = nn.L1Loss() if args.l1_loss else nn.MSELoss()

        self.criterion_geo = GeodesicLoss().to(self.device)
        self.fps = args.data.fps
        if args.bvh_viz:
            self.viz_dir = os.path.join(args.save_dir, "results", "bvh")
        else:
            self.viz_dir = os.path.join(args.save_dir, "results", "smpl")

    def set_input(self, input):
        self.input_data = {
            k: v.float().to(self.device)
            for k, v in input.items()
            if k in ["pos", "global_xform", "root_orient", "root_vel", "trans"]
        }
        self.input_data["rotmat"] = rotation_6d_to_matrix(
            self.input_data["global_xform"]
        )

    def forward(self, step=1):
        b_size, t_length, n_joints = self.input_data["pos"].shape[:3]

        t = torch.arange(start=0, end=t_length, step=step).unsqueeze(0)  # (1, T)
        t = t / t_length * 2 - 1  # map t to [-1, 1]
        t = t.expand(b_size, -1).unsqueeze(-1).to(self.device)  # (B, T, 1)
        f, _ = self.field(t)  # (B, T, D), rot6d + root orient + root vel + root height

        self.recon_data = dict()
        rot6d_recon = f[:, :, : n_joints * 6]  # (B, T, J x 6)
        rot6d_recon = rot6d_recon.view(b_size, -1, n_joints, 6)  # (B, T, J, 6)
        rotmat_recon = rotation_6d_to_matrix(rot6d_recon)  # (B, T, J, 3, 3)
        if self.args.data.root_transform:
            identity = torch.zeros(b_size, t.shape[1], 3, 3).to(self.device)
            identity[:, :, 0, 0] = 1
            identity[:, :, 1, 1] = 1
            identity[:, :, 2, 2] = 1
            rotmat_recon[:, :, 0] = identity
        rotmat_recon[:, :, -2] = rotmat_recon[:, :, -4]
        rotmat_recon[:, :, -1] = rotmat_recon[:, :, -3]
        local_rotmat = self.fk.global_to_local(
            rotmat_recon.view(-1, n_joints, 3, 3)
        )  # (B x T, J, 3. 3)
        pos_recon, _ = self.fk(local_rotmat)  # (B x T, J, 3)
        pos_recon = pos_recon.contiguous().view(b_size, -1, n_joints, 3)  # (B, T, J, 3)
        self.recon_data["rotmat"] = rotmat_recon
        self.recon_data["pos"] = pos_recon

        if self.args.data.root_transform:
            root_orient = f[:, :, n_joints * 6 : n_joints * 6 + 6]  # (B, T, 6)
            self.recon_data["root_orient"] = root_orient
        else:
            self.recon_data["root_orient"] = rot6d_recon[:, :, 0]  # (B, T, 6)

        root_vel = f[:, :, -4:-1]  # (B, T, 3)
        root_height = f[:, :, -1]  # (B, T)

        self.recon_data["root_vel"] = root_vel
        self.recon_data["root_height"] = root_height

        dt = 1.0 / self.args.data.fps * step
        origin = torch.zeros(b_size, 3).to(self.device)
        trans = compute_trajectory(
            root_vel, root_height, origin, dt, up_axis=self.args.data.up
        )
        self.recon_data["trans"] = trans

    def backward(self, validation=False):
        loss = 0

        root_orient = rotation_6d_to_matrix(self.recon_data["root_orient"])
        root_orient_gt = rotation_6d_to_matrix(self.input_data["root_orient"])
        if self.args.geodesic_loss:
            orient_recon_loss = self.criterion_geo(
                root_orient.view(-1, 3, 3), root_orient_gt.view(-1, 3, 3)
            )
        else:
            orient_recon_loss = self.criterion_rec(root_orient, root_orient_gt)
        self.loss_recorder.add_scalar(
            "orient_recon_loss", orient_recon_loss, validation=validation
        )
        loss += self.args.lambda_orient * orient_recon_loss

        if self.args.geodesic_loss:
            rotmat_recon_loss = self.criterion_geo(
                self.recon_data["rotmat"].view(-1, 3, 3),
                self.input_data["rotmat"].view(-1, 3, 3),
            )
        else:
            rotmat_recon_loss = self.criterion_rec(
                self.recon_data["rotmat"], self.input_data["rotmat"]
            )
        self.loss_recorder.add_scalar(
            "rotmat_recon_loss", rotmat_recon_loss, validation=validation
        )
        loss += self.args.lambda_rotmat * rotmat_recon_loss

        pos_recon_loss = self.criterion_rec(
            self.recon_data["pos"], self.input_data["pos"]
        )
        self.loss_recorder.add_scalar(
            "pos_recon_loss", pos_recon_loss, validation=validation
        )
        loss += self.args.lambda_pos * pos_recon_loss

        v_loss = self.criterion_rec(
            self.recon_data["root_vel"], self.input_data["root_vel"]
        )
        up_loss = self.criterion_rec(
            self.recon_data["root_height"],
            self.input_data["trans"][..., "xyz".index(self.args.data.up)],
        )
        self.loss_recorder.add_scalar("v_loss", v_loss, validation=validation)
        self.loss_recorder.add_scalar("up_loss", up_loss, validation=validation)
        loss += self.args.lambda_v * v_loss + self.args.lambda_up * up_loss

        origin = self.input_data["trans"][:, 0]
        trans = self.recon_data["trans"]
        trans[..., self.v_axis] = trans[..., self.v_axis] + origin[
            ..., self.v_axis
        ].unsqueeze(1)
        trans_loss = self.criterion_rec(trans, self.input_data["trans"])
        self.loss_recorder.add_scalar("trans_loss", trans_loss, validation=validation)
        loss += self.args.lambda_trans * trans_loss

        self.loss_recorder.add_scalar("total_loss", loss, validation=validation)

        if not validation:
            loss.backward()

    def optimize_parameters(self):
        self.optimizer.zero_grad()
        self.forward()
        self.backward(validation=False)
        self.optimizer.step()

    def report_errors(self):
        root_orient = rotation_6d_to_matrix(
            self.recon_data["root_orient"]
        )  # (B, T, 3, 3)
        root_orient_gt = rotation_6d_to_matrix(
            self.input_data["root_orient"]
        )  # (B, T, 3, 3)
        orientation_error = self.criterion_geo(
            root_orient.view(-1, 3, 3), root_orient_gt.view(-1, 3, 3)
        )

        local_rotmat = self.fk.global_to_local(
            self.recon_data["rotmat"].view(-1, self.args.smpl.joint_num, 3, 3)
        )  # (B x T, J, 3, 3)
        local_rotmat_gt = self.fk.global_to_local(
            self.input_data["rotmat"].view(-1, self.args.smpl.joint_num, 3, 3)
        )  # (B x T, J, 3, 3)
        rotation_error = self.criterion_geo(
            local_rotmat[:, 1:].reshape(-1, 3, 3),
            local_rotmat_gt[:, 1:].reshape(-1, 3, 3),
        )

        pos = self.recon_data["pos"]  # (B, T, J, 3)
        pos_gt = self.input_data["pos"]  # (B, T, J, 3)
        position_error = torch.linalg.norm((pos - pos_gt), dim=-1).mean()

        origin = self.input_data["trans"][:, 0]
        trans = self.recon_data["trans"]
        trans[..., self.v_axis] = trans[..., self.v_axis] + origin[
            ..., self.v_axis
        ].unsqueeze(1)
        trans_gt = self.input_data["trans"]  # (B, T, 3)
        translation_error = torch.linalg.norm((trans - trans_gt), dim=-1).mean()

        return {
            "rotation": c2c(rotation_error),
            "position": c2c(position_error),
            "orientation": c2c(orientation_error),
            "translation": c2c(translation_error),
        }

    def verbose(self):
        res = {}
        for loss in self.loss_recorder.losses.values():
            res[loss.name] = {"train": loss.current()[0]}

        return res

    def save(self, optimal=False):
        if optimal:
            path = os.path.join(self.args.save_dir, "results", "model")
        else:
            path = os.path.join(self.model_save_dir, f"{self.epoch_cnt:04d}")

        os.makedirs(path, exist_ok=True)
        nemf = (
            self.field.module if isinstance(self.field, nn.DataParallel) else self.field
        )
        torch.save(nemf.state_dict(), os.path.join(path, "nemf.pth"))
        torch.save(self.optimizer.state_dict(), os.path.join(path, "optimizer.pth"))
        if self.args.scheduler.name:
            torch.save(
                self.schedulers[0].state_dict(), os.path.join(path, "scheduler.pth")
            )
        self.loss_recorder.save(path)

        print(f"Save at {path} succeeded")

    def load(self, epoch=None, optimal=False):
        if optimal:
            path = os.path.join(self.args.save_dir, "results", "model")
        else:
            if epoch is None:
                all = [int(q) for q in os.listdir(self.model_save_dir)]
                if len(all) == 0:
                    raise RuntimeError(f"Empty loading path {self.model_save_dir}")
                epoch = sorted(all)[-1]
            path = os.path.join(self.model_save_dir, f"{epoch:04d}")

        print(f"Loading from {path}")
        nemf = (
            self.field.module if isinstance(self.field, nn.DataParallel) else self.field
        )
        nemf.load_state_dict(
            torch.load(os.path.join(path, "nemf.pth"), map_location=self.device)
        )
        if self.is_train:
            self.optimizer.load_state_dict(
                torch.load(os.path.join(path, "optimizer.pth"))
            )
            self.loss_recorder.load(path)
            if self.args.scheduler.name:
                self.schedulers[0].load_state_dict(
                    torch.load(os.path.join(path, "scheduler.pth"))
                )
        self.epoch_cnt = epoch if not optimal else 0

        # print('Load succeeded')

    def super_sampling(self, step=1):
        with torch.no_grad():
            self.forward(step)
            self.fps = self.args.data.fps / step
            self.compute_test_result()

    def compute_test_result(self):
        os.makedirs(self.viz_dir, exist_ok=True)

        rotmat = self.recon_data["rotmat"][0]  # (T, J, 3, 3)
        rotmat_gt = self.input_data["rotmat"][0]  # (T, J, 3, 3)
        local_rotmat = self.fk.global_to_local(rotmat)  # (T, J, 3, 3)
        local_rotmat_gt = self.fk.global_to_local(rotmat_gt)  # (T, J, 3, 3)

        if self.args.data.root_transform:
            root_orient = rotation_6d_to_matrix(
                self.recon_data["root_orient"][0]
            )  # (T, 3, 3)
            root_orient_gt = rotation_6d_to_matrix(
                self.input_data["root_orient"][0]
            )  # (T, 3, 3)
            local_rotmat[:, 0] = root_orient
            local_rotmat_gt[:, 0] = root_orient_gt
        rotation = matrix_to_quaternion(local_rotmat)  # (T, J, 4)
        rotation_gt = matrix_to_quaternion(local_rotmat_gt)  # (T, J, 4)

        origin = self.input_data["trans"][0, 0].clone()  # [3]
        position = self.recon_data["trans"][0].clone()  # (T, 3)
        position[:, self.v_axis] = position[:, self.v_axis] + origin[
            self.v_axis
        ].unsqueeze(0)
        position_gt = self.input_data["trans"][0]  # (T, 3)

        if self.args.bvh_viz:
            if self.args.amass_data:
                rotation = align_joints(rotation)
                rotation_gt = align_joints(rotation_gt)

            position = position.unsqueeze(1)  # (T, 1, 3)
            position_gt = position_gt.unsqueeze(1)  # (T, 1, 3)

            # export data to BVH files
            if self.args.amass_data:
                offsets = self.args.smpl.offsets[self.args.data.gender]
                parents = self.args.smpl.parents
                names = self.args.smpl.joint_names
            else:
                offsets = self.args.animal.offsets
                parents = self.args.animal.parents
                names = self.args.animal.joint_names

            anim = Animation(
                Quaternions(c2c(rotation)),
                c2c(position),
                None,
                offsets=offsets,
                parents=parents,
            )
            BVH.save(
                os.path.join(self.viz_dir, f"test_{int(self.fps)}fps.bvh"),
                anim,
                names=names,
                frametime=1 / self.fps,
            )

            anim.rotations = Quaternions(c2c(rotation_gt))
            anim.positions = c2c(position_gt)
            BVH.save(
                os.path.join(self.viz_dir, "test_gt.bvh"),
                anim,
                names=names,
                frametime=1 / self.args.data.fps,
            )
        else:
            poses = c2c(quaternion_to_axis_angle(rotation))  # (T, J, 3)
            poses_gt = c2c(quaternion_to_axis_angle(rotation_gt))  # (T, J, 3)

            poses = poses.reshape((poses.shape[0], -1))  # (T, J x 3)
            poses = np.pad(poses, [(0, 0), (0, 93)], mode="constant")
            poses_gt = poses_gt.reshape((poses_gt.shape[0], -1))  # (T, J x 3)
            poses_gt = np.pad(poses_gt, [(0, 0), (0, 93)], mode="constant")

            np.savez(
                os.path.join(self.viz_dir, f"test_{int(self.fps)}fps.npz"),
                poses=poses,
                trans=c2c(position),
                betas=np.zeros(10),
                gender=self.args.data.gender,
                mocap_framerate=self.fps,
            )
            np.savez(
                os.path.join(self.viz_dir, "test_gt.npz"),
                poses=poses_gt,
                trans=c2c(position_gt),
                betas=np.zeros(10),
                gender=self.args.data.gender,
                mocap_framerate=self.args.data.fps,
            )
