import math
import os
from abc import ABC, abstractmethod

import torch
import torch.nn.init
import torch.optim


def weights_init(init_type="default"):
    def init_fun(m):
        classname = m.__class__.__name__
        if (classname.find("Conv") == 0 or classname.find("Linear") == 0) and hasattr(
            m, "weight"
        ):
            if init_type == "gaussian":
                torch.nn.init.normal_(m.weight.data)
            elif init_type == "xavier":
                torch.nn.init.xavier_normal_(m.weight.data, gain=math.sqrt(2))
            elif init_type == "kaiming":
                torch.nn.init.kaiming_normal_(m.weight.data, a=0, mode="fan_in")
            elif init_type == "orthogonal":
                torch.nn.init.orthogonal_(m.weight.data, gain=math.sqrt(2))
            elif init_type == "default":
                torch.nn.init.normal_(m.weight.data)
                weight_norm = (
                    m.weight.pow(2)
                    .sum(2, keepdim=True)
                    .sum(1, keepdim=True)
                    .add(1e-8)
                    .sqrt()
                )
                m.weight.data.div_(weight_norm)
            else:
                raise NotImplementedError(f"Unsupported initialization: {init_type}")
            if hasattr(m, "bias") and m.bias is not None:
                torch.nn.init.constant_(m.bias.data, 0.0)

    return init_fun


class BaseModel(ABC):
    """This class is an abstract base class (ABC) for models.
    To create a subclass, you need to implement the following five functions:
        -- <__init__>:                      initialize the class; first call BaseModel.__init__(self, opt).
        -- <set_input>:                     unpack data from dataset and apply preprocessing.
        -- <forward>:                       produce intermediate results.
        -- <optimize_parameters>:           calculate losses, gradients, and update network weights.
    """

    def __init__(self, args):
        self.args = args
        self.is_train = args.is_train
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model_save_dir = os.path.join(
            args.save_dir, "checkpoints"
        )  # save all the checkpoints to save_dir

        if self.is_train and args.log:
            import time
            from torch.utils.tensorboard import SummaryWriter
            from .loss_record import LossRecorder

            log_dir = os.path.join(args.save_dir, "logs")
            if args.epoch_begin != 0:
                all = [d for d in os.listdir(log_dir)]
                if len(all) == 0:
                    raise RuntimeError(f"Empty logging path {log_dir}")
                timestamp = sorted(all)[-1]
            else:
                timestamp = time.strftime(
                    "%Y-%m-%d-%H:%M:%S", time.localtime(time.time())
                )
            self.log_path = os.path.join(log_dir, timestamp)
            os.makedirs(self.log_path, exist_ok=True)
            self.writer = SummaryWriter(self.log_path)
            self.loss_recorder = LossRecorder(self.writer)

        self.epoch_cnt = 0
        self.schedulers = []
        self.optimizers = []
        self.models = []

    @abstractmethod
    def set_input(self, input):
        """Unpack input data from the dataloader and perform necessary pre-processing steps.
        Parameters:
            input (dict): includes the data itself and its metadata information.
        """
        pass

    @abstractmethod
    def compute_test_result(self):
        """
        After forward, do something like output bvh, get error value
        """
        pass

    @abstractmethod
    def forward(self):
        """Run forward pass; called by both functions <optimize_parameters> and <test>."""
        pass

    @abstractmethod
    def optimize_parameters(self):
        """Calculate losses, gradients, and update network weights; called in every training iteration"""
        pass

    def get_scheduler(self, optimizer):
        if self.args.scheduler.name == "linear":

            def lambda_rule(epoch):
                lr_l = 1.0 - max(0, epoch - self.args.n_epochs_origin) / float(
                    self.args.n_epochs_decay + 1
                )
                return lr_l

            return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)
        if self.args.scheduler.name == "StepLR":
            print("StepLR scheduler set")
            return torch.optim.lr_scheduler.StepLR(
                optimizer,
                self.args.scheduler.step_size,
                self.args.scheduler.gamma,
                verbose=self.args.verbose,
            )
        if self.args.scheduler.name == "Plateau":
            print("ReduceLROnPlateau shceduler set")
            return torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode="min",
                factor=self.args.scheduler.factor,
                threshold=self.args.scheduler.threshold,
                patience=5,
                verbose=True,
            )
        if self.args.scheduler.name == "MultiStep":
            print("MultiStep shceduler set")
            return torch.optim.lr_scheduler.MultiStepLR(
                optimizer,
                milestones=self.args.scheduler.milestones,
                gamma=self.args.scheduler.gamma,
                verbose=self.args.verbose,
            )
        else:
            print("No scheduler set")
            return None

    def setup(self):
        """Load and print networks; create schedulers
        Parameters:
            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions
        """
        if self.is_train:
            self.schedulers = [
                self.get_scheduler(optimizer) for optimizer in self.optimizers
            ]

    def epoch(self):
        if self.args.verbose:
            self.loss_recorder.epoch()
        for scheduler in self.schedulers:
            if scheduler is not None:
                if self.args.scheduler.name == "Plateau":
                    loss = self.loss_recorder.losses["total_loss"].current()
                    scheduler.step(loss[1])
                else:
                    scheduler.step()
        self.epoch_cnt += 1

    def test(self):
        """Forward function used in test time.
        This function wraps <forward> function in no_grad() so we don't save intermediate steps for backprop
        It also calls <compute_visuals> to produce additional visualization results
        """
        with torch.no_grad():
            self.forward()
            self.compute_test_result()

    def train(self):
        for model in self.models:
            model.train()
            for param in model.parameters():
                param.requires_grad = True

    def eval(self):
        for model in self.models:
            model.eval()
            for param in model.parameters():
                param.requires_grad = False

    def print(self):
        for model in self.models:
            print(model)

    def count_params(self):
        params = 0
        for model in self.models:
            params += sum(p.numel() for p in model.parameters() if p.requires_grad)

        return params
